{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3067f21c-d8ec-4359-a0ce-2bc1abc4c5c4",
   "metadata": {},
   "source": [
    "# Welcome to the Dask Tutorial\n",
    "\n",
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\">\n",
    "Dask is a parallel and distributed computing library that scales the existing Python and PyData ecosystem.\n",
    "\n",
    "Dask can scale up to your full laptop capacity and out to a cloud cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b5175e-9cee-4e62-a14f-5906ef5a7437",
   "metadata": {},
   "source": [
    "## What is [Dask](https://www.dask.org/)?\n",
    "\n",
    "There are many parts to the \"Dask\" the project:\n",
    "* Collections/API also known as \"core-library\".\n",
    "* Distributed -- to create clusters\n",
    "* Intergrations and broader ecosystem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3588842f-6938-4dbe-b1e8-8d3cb27f57a5",
   "metadata": {},
   "source": [
    "### Dask Collections\n",
    "\n",
    "Dask provides **multi-core** and **distributed+parallel** execution on **larger-than-memory** datasets\n",
    "\n",
    "We can think of Dask's APIs (also called collections)  at a high and a low level:\n",
    "\n",
    "<center>\n",
    "<img src=\"images/high_vs_low_level_coll_analogy.png\" width=\"75%\" alt=\"High vs Low level clothes analogy\">\n",
    "</center>\n",
    "\n",
    "*  **High-level collections:**  Dask provides high-level Array, Bag, and DataFrame\n",
    "   collections that mimic NumPy, lists, and pandas but can operate in parallel on\n",
    "   datasets that don't fit into memory.\n",
    "* **Low-level collections:**  Dask also provides low-level Delayed and Futures\n",
    "   collections that give you finer control to build custom parallel and distributed computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80dc076-98b4-4bf2-adad-3017cd21988d",
   "metadata": {},
   "source": [
    "### Dask Cluster\n",
    "\n",
    "Most of the times when you are using Dask, you will be using a distributed scheduler, which exists in the context of a Dask cluster. The Dask cluster is structured as:\n",
    "\n",
    "<center>\n",
    "<img src=\"images/distributed-overview.png\" width=\"75%\" alt=\"Distributed overview\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69b8da6-70a2-4fd1-9f40-0656d30425d3",
   "metadata": {},
   "source": [
    "### Dask Ecosystem\n",
    "\n",
    "In addition to the core Dask library and its distributed scheduler, the Dask ecosystem connects several additional initiatives, including:\n",
    "\n",
    "- Dask-ML (parallel scikit-learn-style API)\n",
    "- Dask-image\n",
    "- Dask-cuDF\n",
    "- Dask-sql\n",
    "- Dask-snowflake\n",
    "- Dask-mongo\n",
    "- Dask-bigquery\n",
    "\n",
    "Community libraries that have built-in dask integrations like:\n",
    "\n",
    "- Xarray\n",
    "- XGBoost\n",
    "- Prefect\n",
    "- Airflow\n",
    "\n",
    "Dask deployment libraries\n",
    "- Dask-kubernetes\n",
    "- Dask-YARN\n",
    "- Dask-gateway\n",
    "- Dask-cloudprovider\n",
    "- jobqueue\n",
    "\n",
    "... When we talk about the Dask project we include all these efforts as part of the community. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10be9a9b-433f-4919-9597-bb2cef1caa8a",
   "metadata": {},
   "source": [
    "## Dask Use Cases\n",
    "\n",
    "Dask is used in multiple fields such as:\n",
    "\n",
    "* Geospatial\n",
    "* Finance\n",
    "* Astrophysics\n",
    "* Microbiology\n",
    "* Environmental science\n",
    "\n",
    "Check out the Dask [use cases](https://stories.dask.org/en/latest/) page that provides a number of sample workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f02b4b-9c48-406e-9d89-7dbf608bea5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Useful Links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d4aca-f2f3-4e99-994b-1831f1ed10c3",
   "metadata": {},
   "source": [
    "*  Reference\n",
    "    *  [Docs](https://dask.org/)\n",
    "    *  [Examples](https://examples.dask.org/)\n",
    "    *  [Code](https://github.com/dask/dask/)\n",
    "    *  [Blog](https://blog.dask.org/)\n",
    "*  Ask for help\n",
    "    *   [`dask`](http://stackoverflow.com/questions/tagged/dask) tag on Stack Overflow, for usage questions\n",
    "    *   [github issues](https://github.com/dask/dask/issues/new) for bug reports and feature requests\n",
    "    *   [discourse forum](https://dask.discourse.group/) for general, non-bug, questions and discussion\n",
    "    *   Attend a live tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57af8774-1342-4d21-8ed8-25bc2d62cadd",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\">\n",
    "\n",
    "\n",
    "# Dask DataFrame - parallelized pandas\n",
    "\n",
    "Looks and feels like the pandas API, but for parallel and distributed workflows. \n",
    "\n",
    "At its core, the `dask.dataframe` module implements a \"blocked parallel\" `DataFrame` object that looks and feels like the pandas API, but for parallel and distributed workflows. One Dask `DataFrame` is comprised of many in-memory pandas `DataFrame`s separated along the index. One operation on a Dask `DataFrame` triggers many pandas operations on the constituent pandas `DataFrame`s in a way that is mindful of potential parallelism and memory constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2971c498-dc9b-487c-8e46-a109c04c4015",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"https://docs.dask.org/en/stable/_images/dask-dataframe.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask DataFrame is composed of pandas DataFrames\"/>\n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "* [DataFrame documentation](https://docs.dask.org/en/latest/dataframe.html)\n",
    "* [DataFrame screencast](https://youtu.be/AT2XtFehFSQ)\n",
    "* [DataFrame API](https://docs.dask.org/en/latest/dataframe-api.html)\n",
    "* [DataFrame examples](https://examples.dask.org/dataframe.html)\n",
    "* [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/)\n",
    "\n",
    "## When to use `dask.dataframe`\n",
    "\n",
    "pandas is great for tabular datasets that fit in memory. A general rule of thumb for pandas is:\n",
    "\n",
    "> \"Have 5 to 10 times as much RAM as the size of your dataset\"\n",
    ">\n",
    "> ~ Wes McKinney (2017) in [10 things I hate about pandas](https://wesmckinney.com/blog/apache-arrow-pandas-internals/)\n",
    "\n",
    "Here \"size of dataset\" means dataset size on _the disk_.\n",
    "\n",
    "Dask becomes useful when the datasets exceed the above rule.\n",
    "\n",
    "In this notebook, you will be working with the New York City Airline data. This dataset is only ~200MB, so that you can download it in a reasonable time, but `dask.dataframe` will scale to  datasets **much** larger than memory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a68bfe1-6b66-47d6-831c-f14efc8dd3f2",
   "metadata": {},
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0702886-51ba-4a97-8e5c-234846e5a1a3",
   "metadata": {},
   "source": [
    "Create the datasets you will be using in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfb07fc-7336-4029-846a-710edeb7b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run prep.py -d flights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8ec851-670d-466b-9979-adc3e7cc1996",
   "metadata": {},
   "source": [
    "## Set up your local cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c7c18f-6750-49e1-a80d-18dd13e76524",
   "metadata": {},
   "source": [
    "Create a local Dask cluster and connect it to the client. Don't worry about this bit of code for now, you will learn more in the Distributed notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8c7ce2-97be-4f63-8d15-3e3490667382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=4)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d089681-8592-4333-a6fb-3b0b19edc09b",
   "metadata": {},
   "source": [
    "### Dask Diagnostic Dashboard\n",
    "\n",
    "Dask Distributed provides a useful Dashboard to visualize the state of your cluster and computations.\n",
    "\n",
    "If you're on **JupyterLab or Binder**, you can use the [Dask JupyterLab extension](https://github.com/dask/dask-labextension) (which should be already installed in your environment) to open the dashboard plots:\n",
    "* Click on the Dask logo in the left sidebar\n",
    "* Click on the magnifying glass icon, which will automatically connect to the active dashboard (if that doesn't work, you can type/paste the dashboard link http://127.0.0.1:8787 in the field)\n",
    "* Click on **\"Task Stream\"**, **\"Progress Bar\"**, and **\"Worker Memory\"**, which will open these plots in new tabs\n",
    "* Re-organize the tabs to suit your workflow!\n",
    "\n",
    "Alternatively, click on the dashboard link displayed in the Client details above: http://127.0.0.1:8787/status. It will open a new browser tab with the Dashboard.\n",
    "\n",
    "**Note**: if you are running Dask on a machine that doesn't have a public IP, you can use the included `jupyter-server-proxy` to access the dashboard.\n",
    "Example URL for dashboard behind proxy: **http://127.0.0.1:<jupyter-port\\>/proxy/<dashboard-port\\>/status**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a54f68-16d7-44f7-bd75-52b17b916856",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reading and working with datasets\n",
    "\n",
    "Let's read an extract of flights in the USA across several years. This data is specific to flights out of the three airports in the New York City area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a91322f-8695-4bc5-9d3d-0120df4be949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea0e1e5-1bc6-41e4-a4a9-2f1f6b8b0a1e",
   "metadata": {},
   "source": [
    "By convention, we import the module `dask.dataframe` as `dd`, and call the corresponding `DataFrame` object `ddf`.\n",
    "\n",
    "**Note**: The term \"Dask DataFrame\" is slightly overloaded. Depending on the context, it can refer to the module or the DataFrame object. To avoid confusion, throughout this notebook:\n",
    "- `dask.dataframe` (note the all lowercase) refers to the API, and\n",
    "- `DataFrame` (note the CamelCase) refers to the object.\n",
    "\n",
    "The following filename includes a glob pattern `*`, so all files in the path matching that pattern will be read into the same `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a342f952-cec0-407b-867c-c05dac3f9044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "ddf = dd.read_csv(\n",
    "    os.path.join(\"data\", \"nycflights\", \"*.csv\"), parse_dates={\"Date\": [0, 1, 2]},\n",
    ")\n",
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384e8494-7720-4fae-a722-4e98baca9c43",
   "metadata": {},
   "source": [
    "Dask has not loaded the data yet, it has:\n",
    "- investigated the input path and found that there are ten matching files\n",
    "- intelligently created a set of jobs for each chunk -- one per original CSV file in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d3774e-c23f-495e-8d87-ee276e8d296c",
   "metadata": {},
   "source": [
    "Notice that the representation of the `DataFrame` object contains no data - Dask has just done enough to read the start of the first file, and infer the column names and dtypes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716b9cf6-ba68-4085-8454-a12e7ea85e92",
   "metadata": {},
   "source": [
    "### Lazy Evaluation\n",
    "\n",
    "Most Dask Collections, including Dask `DataFrame` are evaluated lazily, which means Dask constructs the logic (called task graph) of your computation immediately but \"evaluates\" them  only when necessary. You can view this task graph using `.visualize()`.\n",
    "\n",
    "You will learn more about this in the Delayed notebook, but for now, note that we need to call `.compute()` to trigger actual computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1e8992-ac7b-4b27-9a9c-0947ce2afe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261015da-e514-46f2-b7b7-53710bb941c6",
   "metadata": {},
   "source": [
    "Some functions like `len` and `head` also trigger a computation. Specifically, calling `len` will:\n",
    "- load actual data, (that is, load each file into a pandas DataFrame)\n",
    "- then apply the corresponding functions to each pandas DataFrame (also known as a partition)\n",
    "- combine the subtotals to give you the final grand total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f066a937-08c5-4283-9525-23b4a3958e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and count number of rows\n",
    "len(ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865af021-66ad-401e-ba68-2c905cb072e3",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "source": [
    "```python\n",
    "len(ddf)\n",
    "\n",
    "# ValueError: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n",
    "\n",
    "# +----------------+---------+----------+\n",
    "# | Column         | Found   | Expected |\n",
    "# +----------------+---------+----------+\n",
    "# | CRSElapsedTime | float64 | int64    |\n",
    "# | TailNum        | object  | float64  |\n",
    "# +----------------+---------+----------+\n",
    "\n",
    "# The following columns also raised exceptions on conversion:\n",
    "\n",
    "# - TailNum\n",
    "#   ValueError(\"could not convert string to float: 'N54711'\")\n",
    "\n",
    "# Usually this is due to dask's dtype inference failing, and\n",
    "# *may* be fixed by specifying dtypes manually by adding:\n",
    "\n",
    "# dtype={'CRSElapsedTime': 'float64',\n",
    "#        'TailNum': 'object'}\n",
    "\n",
    "# to the call to `read_csv`/`read_table`.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9898af-82c9-4c04-b95e-2133988da908",
   "metadata": {},
   "source": [
    "Unlike `pandas.read_csv` which reads in the entire file before inferring datatypes, `dask.dataframe.read_csv` only reads in a sample from the beginning of the file (or first file if using a glob). These inferred datatypes are then enforced when reading all partitions.\n",
    "\n",
    "In this case, the datatypes inferred in the sample are incorrect. The first `n` rows have no value for `CRSElapsedTime` (which pandas infers as a `float`), and later on turn out to be strings (`object` dtype). Note that Dask gives an informative error message about the mismatch. When this happens you have a few options:\n",
    "\n",
    "- Specify dtypes directly using the `dtype` keyword. This is the recommended solution, as it's the least error prone (better to be explicit than implicit) and also the most performant.\n",
    "- Increase the size of the `sample` keyword (in bytes)\n",
    "- Use `assume_missing` to make `dask` assume that columns inferred to be `int` (which don't allow missing values) are actually `floats` (which do allow missing values). In our particular case this doesn't apply.\n",
    "\n",
    "In our case we'll use the first option and directly specify the `dtypes` of the offending columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3881fcb3-2976-4315-928d-c905a16efa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_csv(\n",
    "    os.path.join(\"data\", \"nycflights\", \"*.csv\"),\n",
    "    parse_dates={\"Date\": [0, 1, 2]},\n",
    "    dtype={\"TailNum\": str, \"CRSElapsedTime\": float, \"Cancelled\": bool},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d826b4-43e9-4c39-97c7-fb580c5e2348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and count number of rows\n",
    "len(ddf)  # now works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de57faa5-8916-4f99-9d6f-f5c1c8e9d386",
   "metadata": {},
   "source": [
    "You can view the start and end of the data as you would in pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d424bfb4-0e9d-4e5f-8d90-165079c717df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a94352-77eb-4212-8e13-83bf6de97652",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.tail() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c46658-886b-44c9-984f-3bba638ceea6",
   "metadata": {},
   "source": [
    "### Reading from remote storage\n",
    "\n",
    "If you're thinking about distributed computing, your data is probably stored remotely on services (like Amazon's S3 or Google's cloud storage) and is in a friendlier format (like Parquet). Dask can read data in various formats directly from these remote locations **lazily** and **in parallel**.\n",
    "\n",
    "Here's how you can read the NYC taxi cab data from Amazon S3:\n",
    "\n",
    "```python\n",
    "ddf = dd.read_parquet(\n",
    "    \"s3://nyc-tlc/trip data/yellow_tripdata_2012-*.parquet\",\n",
    ")\n",
    "```\n",
    "\n",
    "You can also leverage Parquet-specific optimizations like column selection and metadata handling, learn more in [the Dask documentation on working with Parquet files](https://docs.dask.org/en/stable/dataframe-parquet.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabaffba-291f-4e1f-a1f7-2c6199715dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_nyc = dd.read_parquet(\n",
    "    \"s3://dask-data/nyc-taxi/nyc-2015.parquet/part.*.parquet\",\n",
    "    columns=[\"passenger_count\", \"tip_amount\"],\n",
    "    storage_options={\"anon\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9652d4b0-6028-4d3d-a2f0-5e08ebe5fe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ddf_nyc.groupby(\"passenger_count\").tip_amount.mean().compute()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01d35c6-c8c6-4078-9820-088fc88d9a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_nyc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db5876e-9fdc-4f71-9a00-b9a19924eada",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(ddf_nyc.groupby(\"passenger_count\").tip_amount.mean()).visualize(engine='cytoscape')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3ddf9f-8527-4499-801e-23b38b4c4daa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Computations with `dask.dataframe`\n",
    "\n",
    "Let's compute the maximum of the flight delay.\n",
    "\n",
    "With just pandas, we would loop over each file to find the individual maximums, then find the final maximum over all the individual maximums.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "files = os.listdir(os.path.join('data', 'nycflights'))\n",
    "\n",
    "maxes = []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(os.path.join('data', 'nycflights', file))\n",
    "    maxes.append(df.DepDelay.max())\n",
    "    \n",
    "final_max = max(maxes)\n",
    "```\n",
    "\n",
    "`dask.dataframe` lets us write pandas-like code, that operates on larger-than-memory datasets in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6797cde-6baf-4009-a9dd-57c668f2ff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = ddf.DepDelay.max()\n",
    "print(result.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23968d73-72b5-4072-9815-5cf8fb891bab",
   "metadata": {},
   "source": [
    "This creates the lazy computation for us and then runs it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07920592-04e9-4bf5-abe6-d579bdda961d",
   "metadata": {},
   "source": [
    "**Note:** Dask will delete intermediate results (like the full pandas DataFrame for each file) as soon as possible. This means you can handle datasets that are larger than memory but, repeated computations will have to load all of the data in each time. (Run the code above again, is it faster or slower than you would expect?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd1778c-810c-43b6-915d-eaf48ed38ba3",
   "metadata": {},
   "source": [
    "You can view the underlying task graph using `.visualize()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3127fc25-b863-4cdf-9143-318241a1f94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the parallelism\n",
    "result.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018e5992-a54a-4255-b5f5-8e4fe897994f",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "In this section you will do a few `dask.dataframe` computations. If you are comfortable with pandas then these should be familiar. You will have to think about when to call `.compute()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b714d431-2f66-40c6-86bb-4917a43058ac",
   "metadata": {},
   "source": [
    "### 1. How many rows are in our dataset?\n",
    "\n",
    "_Hint_: how would you check how many items are in a list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c843392-f269-4850-9da8-147e161dac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569d6db0-86aa-4170-97e8-00a0997cc2e8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "len(ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e72fbd4-ab76-45d7-9a38-2c66d8090cab",
   "metadata": {},
   "source": [
    "### 2. In total, how many non-canceled flights were taken?\n",
    "\n",
    "_Hint_: use [boolean indexing](https://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00bc6aa-898a-4f8f-9ec0-d8ccf8be09d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a1d61c-a0bb-48c3-acd9-1b326d58487c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "len(ddf[~ddf.Cancelled])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ec53ab-68a2-4f4f-8410-a89e348ab052",
   "metadata": {},
   "source": [
    "### 3. In total, how many non-canceled flights were taken from each airport?\n",
    "\n",
    "*Hint*: use [groupby](https://pandas.pydata.org/pandas-docs/stable/groupby.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6ae135-fa01-4e85-b5c7-89aba4460106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58467b5-b781-44a1-8379-168d6ed99173",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "ddf[~ddf.Cancelled].groupby(\"Origin\").Origin.count().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52129783-e45d-4d3c-a4fd-30a3f5ee0fd1",
   "metadata": {},
   "source": [
    "### 4. What was the average departure delay from each airport?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec98e4a-6c24-4906-a90b-96cb67902284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfe45f3-18de-401f-bd03-8a9b2f39a373",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "ddf.groupby(\"Origin\").DepDelay.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a43178f-d2dd-49cb-bb65-679d9db1268c",
   "metadata": {},
   "source": [
    "### 5. What day of the week has the worst average departure delay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5538b7b3-e030-46dc-995a-2eadf8aa80cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2efde49-79d7-4d89-85fe-88418f213a8c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "ddf.groupby(\"DayOfWeek\").DepDelay.mean().idxmax().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608ef020-d1b6-4b15-8ab5-264a8cb65c80",
   "metadata": {},
   "source": [
    "### 6. Let's say the distance column is erroneous and you need to add 1 to all values, how would you do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de890002-985e-475a-b831-1035a62415a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bc3b76-e40e-4072-ad98-46d60bef0c3c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "ddf[\"Distance\"].apply(\n",
    "    lambda x: x + 1\n",
    ").compute()  # don't worry about the warning, we'll discuss in the next sections\n",
    "\n",
    "# OR\n",
    "\n",
    "(ddf[\"Distance\"] + 1).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989e25ab-0d93-4c8a-99ea-8668f00e0fc0",
   "metadata": {},
   "source": [
    "## Sharing Intermediate Results\n",
    "\n",
    "When computing all of the above, we sometimes did the same operation more than once. For most operations, `dask.dataframe` stores the arguments, allowing duplicate computations to be shared and only computed once.\n",
    "\n",
    "For example, let's compute the mean and standard deviation for departure delay of all non-canceled flights. Since Dask operations are lazy, those values aren't the final results yet. They're just the steps required to get the result.\n",
    "\n",
    "If you compute them with two calls to compute, there is no sharing of intermediate computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd735e8-88dd-4103-87c1-a25c52904881",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_canceled = ddf[~ddf.Cancelled]\n",
    "mean_delay = non_canceled.DepDelay.mean()\n",
    "std_delay = non_canceled.DepDelay.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ee5bf3-9f1c-4201-9b0f-f56dbf4d53b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mean_delay_res = mean_delay.compute()\n",
    "std_delay_res = std_delay.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6750c6-fc81-4764-8f22-ea374b040943",
   "metadata": {},
   "source": [
    "### `dask.compute`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20d5107-7c09-4e84-ac45-49026b681026",
   "metadata": {},
   "source": [
    "But let's try by passing both to a single `compute` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd2bb5-6c40-445c-9575-f278e9af9551",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mean_delay_res, std_delay_res = dask.compute(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f29f4f0-0f55-413f-984c-e570fff223e2",
   "metadata": {},
   "source": [
    "Using `dask.compute` takes roughly 1/2 the time. This is because the task graphs for both results are merged when calling `dask.compute`, allowing shared operations to only be done once instead of twice. In particular, using `dask.compute` only does the following once:\n",
    "\n",
    "- the calls to `read_csv`\n",
    "- the filter (`df[~df.Cancelled]`)\n",
    "- some of the necessary reductions (`sum`, `count`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a9568d-9dc9-40d4-a6a0-c5abb42c8f1d",
   "metadata": {},
   "source": [
    "To see what the merged task graphs between multiple results look like (and what's shared), you can use the `dask.visualize` function (you might want to use `filename='graph.pdf'` to save the graph to disk so that you can zoom in more easily):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b809feec-572e-4672-9f49-0acd6fa84adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(mean_delay, std_delay, engine=\"cytoscape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adf7791-52b8-44ff-8c5f-bd34e1d70249",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `.persist()`\n",
    "\n",
    "While using a distributed scheduler (you will learn more about schedulers in the upcoming tutorials), you can keep some _data that you want to use often_ in the _distributed memory_. \n",
    "\n",
    "`persist` generates \"Futures\" (more on this later as well) and stores them in the same structure as your output. You can use `persist` with any data or computation that fits in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78eb690-512f-452f-af63-e2ac6dcf7665",
   "metadata": {},
   "source": [
    "If you want to analyze data only for non-canceled flights departing from JFK airport, you can either have two compute calls like in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd48172-ffc1-4a1e-ba96-75fbb5ba715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_cancelled = ddf[~ddf.Cancelled]\n",
    "ddf_jfk = non_cancelled[non_cancelled.Origin == \"JFK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81a2fb9-15c9-4f4d-9606-720104e33791",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf_jfk.DepDelay.mean().compute()\n",
    "ddf_jfk.DepDelay.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31af9df-066f-429a-b2e7-5ed287a26962",
   "metadata": {},
   "source": [
    "Or, consider persisting that subset of data in memory.\n",
    "\n",
    "See the \"Graph\" dashboard plot, the red squares indicate persisted data stored as Futures in memory. You will also notice an increase in Worker Memory (another dashboard plot) consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac7b43a-04f3-467b-8c74-232cac031b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_jfk = ddf_jfk.persist()  # returns back control immediately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649c1a71-52d4-4ece-b717-b9986b59bc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf_jfk.DepDelay.mean().compute()\n",
    "ddf_jfk.DepDelay.std().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95509b8-87c0-4b59-8e60-dad7ea96bcc9",
   "metadata": {},
   "source": [
    "Analyses on this persisted data is faster because we are not repeating the loading and selecting (non-canceled, JFK departure) operations.\n",
    "\n",
    "To remove this from memory, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dbc2ca-5bc5-4e25-a136-6253a5af4e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ddf_jfk\n",
    "# or: client.cancel(ddf_jfk), but this removes any scheduled jobs and all jobs/pointers to this result!\n",
    "# The proper term instead of \"jobs\" would be \"futures\", but we'll get to that later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c641fc-f2da-4c36-a506-89fb01d7f3ef",
   "metadata": {},
   "source": [
    "See https://distributed.dask.org/en/stable/memory.html#clearing-data for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4df241-d3e3-4a31-980e-ee34e3c6c337",
   "metadata": {},
   "source": [
    "## Custom code with Dask DataFrame\n",
    "\n",
    "`dask.dataframe` only covers a small but well-used portion of the pandas API.\n",
    "\n",
    "This limitation is for two reasons:\n",
    "\n",
    "1.  The Pandas API is *huge*\n",
    "2.  Some operations are genuinely hard to do in parallel, e.g, sorting.\n",
    "\n",
    "Additionally, some important operations like `set_index` work, but are slower than in pandas because they include substantial shuffling of data, and may write out to disk.\n",
    "\n",
    "**What if you want to use some custom functions that aren't (or can't be) implemented for Dask DataFrame yet?**\n",
    "\n",
    "You can open an issue on the [Dask issue tracker](https://github.com/dask/dask/issues) to check how feasible the function could be to implement, and you can consider contributing this function to Dask.\n",
    "\n",
    "In case it's a custom function or tricky to implement, `dask.dataframe` provides a few methods to make applying custom functions to Dask DataFrames easier:\n",
    "\n",
    "- [`map_partitions`](https://docs.dask.org/en/latest/generated/dask.dataframe.DataFrame.map_partitions.html): to run a function on each partition (each pandas DataFrame) of the Dask DataFrame\n",
    "- [`map_overlap`](https://docs.dask.org/en/latest/generated/dask.dataframe.rolling.map_overlap.html): to run a function on each partition (each pandas DataFrame) of the Dask DataFrame, with some rows shared between neighboring partitions\n",
    "- [`reduction`](https://docs.dask.org/en/latest/generated/dask.dataframe.Series.reduction.html): for custom row-wise reduction operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b2129e-77a1-400a-929a-900a065fc548",
   "metadata": {},
   "source": [
    "Let's take a quick look at the `map_partitions()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b731a8c3-58f3-42f9-aa57-6609188a1c3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "help(ddf.map_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa0c837-c392-4f7e-bd21-1fef8c7c3c8f",
   "metadata": {},
   "source": [
    "The \"Distance\" column in `ddf` is currently in miles. Let's say we want to convert the units to kilometers and we have a general helper function as shown below. In this case, we can use `map_partitions` to apply this function across each of the internal pandas `DataFrame`s in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855e0fe2-1598-40a7-ac9b-78dd1d8acf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_custom_converter(df, multiplier=1):\n",
    "    return df * multiplier\n",
    "\n",
    "\n",
    "meta = pd.Series(name=\"Distance\", dtype=\"float64\")\n",
    "\n",
    "distance_km = ddf.Distance.map_partitions(\n",
    "    my_custom_converter, multiplier=0.6, meta=meta\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68917e8-36b0-40f8-b14d-49874d550f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_km.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a98170-52c1-420e-b0f0-41cd4e9cf156",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_km.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74da924e-57ef-4a40-8feb-ca885a8657af",
   "metadata": {},
   "source": [
    "### What is `meta`?\n",
    "\n",
    "Since Dask operates lazily, it doesn't always have enough information to infer the output structure (which includes datatypes) of certain operations.\n",
    "\n",
    "`meta` is a _suggestion_ to Dask about the output of your computation. Importantly, `meta` _never infers with the output structure_. Dask uses this `meta` until it can determine the actual output structure.\n",
    "\n",
    "Even though there are many ways to define `meta`, we suggest using a small pandas Series or DataFrame that matches the structure of your final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3dc598-30bd-4fb4-93ba-524ba661d556",
   "metadata": {},
   "source": [
    "It's good practice to always close any Dask cluster you create:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977f611a-20a6-475f-819d-2d4072fa4398",
   "metadata": {},
   "source": [
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\">\n",
    "\n",
    "# Dask Arrays - parallelized numpy\n",
    "Parallel, larger-than-memory, n-dimensional array using blocked algorithms. \n",
    "\n",
    "*  **Parallel**: Uses all of the cores on your computer\n",
    "*  **Larger-than-memory**:  Lets you work on datasets that are larger than your available memory by breaking up your array into many small pieces, operating on those pieces in an order that minimizes the memory footprint of your computation, and effectively streaming data from disk.\n",
    "*  **Blocked Algorithms**:  Perform large computations by performing many smaller computations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ab060-276c-4a58-aa0b-5b0308e90c67",
   "metadata": {},
   "source": [
    "<img src=\"https://docs.dask.org/en/stable/_images/dask-array.svg\" width=\"40%\" align=\"right\">\n",
    "\n",
    "\n",
    "In other words, Dask Array implements a subset of the NumPy ndarray interface using blocked algorithms, cutting up the large array into many small arrays. This lets us compute on arrays larger than memory using all of our cores. We coordinate these blocked algorithms using Dask graphs.\n",
    "\n",
    "In this notebook, we'll build some understanding by implementing some blocked algorithms from scratch.\n",
    "We'll then use Dask Array to analyze large datasets, in parallel, using a familiar NumPy-like API.\n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "* [Array documentation](https://docs.dask.org/en/latest/array.html)\n",
    "* [Array screencast](https://youtu.be/9h_61hXCDuI)\n",
    "* [Array API](https://docs.dask.org/en/latest/array-api.html)\n",
    "* [Array examples](https://examples.dask.org/array.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3aec8a-2e31-45b4-88b7-9652aacf3a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse client\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7d00d-f402-4f04-8b64-ff92dcfaadd5",
   "metadata": {},
   "source": [
    "## Blocked Algorithms in a nutshell\n",
    "\n",
    "Let's do side by side the sum of the elements of an array using a NumPy array and a Dask array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e078df7-062e-477a-8b7b-1fcbcd5a8a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927e9265-cf46-4362-8d56-57c5cdecc968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy array\n",
    "a_np = np.ones(10)\n",
    "a_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7054c267-c732-4ff0-b49f-f26d9da8f7b8",
   "metadata": {},
   "source": [
    "We know that we can use `sum()` to compute the sum of the elements of our array, but to show what a blocksized operation would look like, let's do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0658204-d628-4ed3-8f53-c656410fe282",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_np_sum = a_np[:5].sum() + a_np[5:].sum()\n",
    "a_np_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671652b6-4eb2-4ed0-8b00-219e9e7f12ea",
   "metadata": {},
   "source": [
    "Now notice that each sum in the computation above is completely independent so they could be done in parallel. \n",
    "To do this with Dask array, we need to define our \"slices\", we do this by defining the amount of elements we want per block using the variable `chunks`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ee3527-f8c8-4731-a8ec-e66de9d4008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_da = da.ones(10, chunks=5)\n",
    "a_da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b650158-c720-4bf9-959c-3c4a23f8733e",
   "metadata": {},
   "source": [
    "**Important!**\n",
    "\n",
    "Note here that to get two blocks, we specify `chunks=5`, in other words, we have 5 elements per block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49c07c4-ad46-4b43-85b3-92148a1032d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_da_sum = a_da.sum()\n",
    "a_da_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be377c29-bb51-44cf-9775-4762813ee978",
   "metadata": {},
   "source": [
    "## Task Graphs\n",
    "\n",
    "In general, the code that humans write rely on compilers or interpreters so the computers can understand what we wrote. When we move to parallel execution there is a desire to shift responsibility from the compilers to the human, as they often bring the analysis, optimization, and execution of code into the code itself. In these cases, we often represent the structure of our program explicitly as data within the program itself.\n",
    "\n",
    "In Dask we use task scheduling, where we break our program into into many medium-sized tasks or units of computation.We represent these tasks as nodes in a graph with edges between nodes if one task depends on data produced by another. We call upon a task scheduler to execute this graph in a way that respects these data dependencies and leverages parallelism where possible, so multiple independent tasks can be run simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5068ed-2cba-46e9-a24d-cffb5c6b2930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the low level Dask graph using cytoscape\n",
    "a_da_sum.visualize(engine=\"cytoscape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb81599-1ead-4789-8532-37b64712dee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_da_sum.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b5f089-b309-427a-a026-838a3baa979d",
   "metadata": {},
   "source": [
    "Performance comparison\n",
    "------------------------------\n",
    "\n",
    "Let's try a more interesting example. We will create a 20_000 x 20_000 array with normally distributed values, and take the mean along one of its axis.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "If you are running on Binder, the Numpy example might need to be a smaller one due to memory issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd351e8-747d-4f36-a885-b0c1f8ef09d4",
   "metadata": {},
   "source": [
    "### Numpy version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6f6ac3-dfef-4fd6-9920-5b6e7cc64000",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xn = np.random.normal(10, 0.1, size=(10_000, 10_000))\n",
    "yn = xn.mean(axis=0)\n",
    "yn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eafd265-fe03-45db-8811-33ec28e79020",
   "metadata": {},
   "source": [
    "### Dask array version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f3213b-7d4a-477c-8dee-fb8a09a0604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xd = da.random.normal(10, 0.1, size=(30_000, 30_000), chunks=(3000, 3000))\n",
    "xd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c90121-6f16-41c3-ab63-ee37f70e765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xd.nbytes / 1e9  # Gigabytes of the input processed lazily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa33bfb-5255-4789-9ed5-d2f7839e69e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "yd = xd.mean(axis=0)\n",
    "yd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e38900e-6058-40a2-a945-11451b68d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xd = da.random.normal(10, 0.1, size=(30_000, 30_000), chunks=(3000, 3000))\n",
    "yd = xd.mean(axis=0)\n",
    "yd.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0806fa-a334-49ba-a34f-735e55385119",
   "metadata": {},
   "source": [
    "**Questions to think about:**\n",
    "\n",
    "* What happens if the Dask chunks=(10000,10000)?\n",
    "* What happens if the Dask chunks=(30,30)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbacc75c-6c4e-4a23-b5d8-470765d3b62d",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "\n",
    "For Dask arrays, compute the mean along `axis=1` of the sum of the x array and its transpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba025e1-4f76-4036-b56a-b5384003a30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b3bc8-0987-4ca6-bdcf-620710d95b63",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "x_sum = xd + xd.T\n",
    "res = x_sum.mean(axis=1)\n",
    "res.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276624eb-8ec9-4541-8f0a-a12dc22b9939",
   "metadata": {},
   "source": [
    "## Choosing good chunk sizes\n",
    "This section was inspired on a Dask blog by Genevieve Buckley you can read it [here](https://blog.dask.org/2021/11/02/choosing-dask-chunk-sizes)\n",
    "\n",
    "A common problem when getting started with Dask array is determine what is a good chunk size. But what is a good size, and how do we determine this? \n",
    "\n",
    "\n",
    "### Get to know the chunks \n",
    "\n",
    "We can think of Dask arrays as a big structure composed by chunks of a smaller size, where these chunks are typically an a single `numpy` array, and they are all arranged to form a larger Dask array. \n",
    "\n",
    "If you have a Dask array and want to know more information about chunks and their size, you can use the `chunksize` and `chunks` attributes to access this information. If you are in a jupyter notebook\n",
    "you can also visualize the Dask array via its HTML representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff33cb11-c630-4f02-89c2-9999d1cf1c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "darr = da.random.random((1000, 1000, 1000))\n",
    "darr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0015fe30-044e-4969-91bb-e389704fbc13",
   "metadata": {},
   "source": [
    "Notice that when we created the Dask array, we did not specify the `chunks`. Dask has set by default `chunks='auto'` which accommodates ideal chunk  sizes. To learn more on how auto-chunking works you can go to this documentation https://docs.dask.org/en/stable/array-chunks.html#automatic-chunking\n",
    "\n",
    "`darr.chunksize` shows the largest chunk size. If you expect your array to have uniform chunk sizes this is a a good summary of the chunk size information. But if your array have irregular chunks, `darr.chunks` will show you the explicit sizes of all the chunks along all the dimensions of your dask array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe13f6-c52c-4982-9dfd-003fa08f57f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "darr.chunksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef62faf-17c4-4a43-a23f-20a7aba73427",
   "metadata": {},
   "outputs": [],
   "source": [
    "darr.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb849f5-5772-4775-90a0-8e510aa0a73c",
   "metadata": {},
   "source": [
    "Let's modify our example to see explore chunking a bit more. We can rechunk our array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b280fdf-c509-4560-b899-06ecd6997e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "darr = darr.rechunk({0: -1, 1: 100, 2: \"auto\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed425741-df67-4a12-952f-d2c722f80edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "darr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0c4861-ad66-49a7-aac0-46ed72f2a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "darr.chunksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06e85d1-e214-4fff-b9fb-d5668ee4b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "darr.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e864b0fe-20d9-4c0a-8655-47a48bfb04e0",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "- What does -1 do when specify as the chunk on a certain axis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4408b11f-8b7a-40e9-bcbd-e53e1a51a0b4",
   "metadata": {},
   "source": [
    "### Too small is a problem\n",
    "\n",
    "If your chunks are too small, the amount of actual work done by every task is very tiny, and the overhead of coordinating all these tasks results in a very inefficient process. \n",
    "\n",
    "In general, the dask scheduler takes approximately one millisecond to coordinate a single task. That means we want the computation time to be comparatively large, i.e in the order of seconds. \n",
    "\n",
    "Intuitive analogy by Genevieve Buckley:\n",
    "\n",
    "> Lets imagine we are building a house. It is a pretty big job, and if there were only one worker it would take much too long to build. So we have a team of workers and a site foreman. The site foreman is equivalent to the Dask scheduler: their job is to tell the workers what tasks they need to do.  \n",
    "Say we have a big pile of bricks to build a wall, sitting in the corner of the building site. If the foreman (the Dask scheduler) tells workers to go and fetch a single brick at a time, then bring each one to where the wall is being built, you can see how this is going to be very slow and inefficient! The workers are spending most of their time moving between the wall and the pile of bricks. Much less time is going towards doing the actual work of mortaring bricks onto the wall.  \n",
    "Instead, we can do this in a smarter way. The foreman (Dask scheduler) can tell the workers to go and bring one full wheelbarrow load of bricks back each time. Now workers are spending much less time moving between the wall and the pile of bricks, and the wall will be finished much quicker. \n",
    "\n",
    "### Too big is a problem\n",
    "\n",
    "If your chunks are too big, this is also a problem because you will likely run out of memory. You will start seeing in the dashboard that data is being spill to disk and this will lead to performance decrements. \n",
    "\n",
    "If we load to much data into memory, Dask workers will start to spill data to disk to avoid crashing. Spilling data to disk will slow things down significantly, because of all the extra read and write operations to disk. This is definitely a situation that we want to avoid, to watch out for this you can look at the worker memory plot on the dashboard. Orange bars are a warning you are close to the limit, and gray means data is being spilled to disk. \n",
    "\n",
    "To watch out for this, look at the worker memory plot on the Dask dashboard. Orange bars are a warning you are close to the limit, and gray means data is being spilled to disk - not good! For more tips, see the section on using the Dask dashboard below. To learn more about the memory plot, check the [dashboard documentation](https://docs.dask.org/en/stable/dashboard.html#bytes-stored-and-bytes-per-worker).\n",
    "\n",
    "\n",
    "### Rules of thumb\n",
    "\n",
    "- Users have reported that chunk sizes smaller than 1MB tend to be bad. In general, a chunk size between **100MB and 1GB is good**, while going over 1 or 2GB means you have a really big dataset and/or a lot of memory available per worker.\n",
    "- Upper bound: Avoid very large task graphs. More than 10,000 or 100,000 chunks may start to perform poorly.\n",
    "- Lower bound: To get the advantage of parallelization, you need the number of chunks to at least equal the number of worker cores available (or better, the number of worker cores times 2). Otherwise, some workers will stay idle.\n",
    "- The time taken to compute each task should be much larger than the time needed to schedule the task. The Dask scheduler takes roughly 1 millisecond to coordinate a single task, so a good task computation time would be in the order of seconds (not milliseconds).\n",
    "- Chunks should be aligned with array storage on disk. Modern NDArray storage formats (HDF5, NetCDF, TIFF, Zarr) allow arrays to be stored in chunks so that the blocks of data can be pulled efficiently. However, data stores often chunk more finely than is ideal for Dask array, so it is common to choose a chunking that is a multiple of your storage chunk size, otherwise you might incur high overhead. For example,  if you are loading data that is chunked in blocks of (100, 100), the  you might might choose a chunking strategy more like (1000, 2000) that is larger but still divisible by (100, 100). \n",
    "\n",
    "For more more advice on chunking see https://docs.dask.org/en/stable/array-chunks.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db988bd-1413-42af-b455-312413de607b",
   "metadata": {},
   "source": [
    "<img src=\"https://docs.dask.org/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\">\n",
    "\n",
    "# dask.delayed - parallelize any code\n",
    "\n",
    "What if you don't have an array or dataframe? Instead of having blocks where the function is applied to each block, you can decorate functions with `@delayed` and _have the functions themselves be lazy_. \n",
    "\n",
    "This is a simple way to use `dask` to parallelize existing codebases or build [complex systems](https://blog.dask.org/2018/02/09/credit-models-with-dask). \n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "* [Delayed documentation](https://docs.dask.org/en/latest/delayed.html)\n",
    "* [Delayed screencast](https://www.youtube.com/watch?v=SHqFmynRxVU)\n",
    "* [Delayed API](https://docs.dask.org/en/latest/delayed-api.html)\n",
    "* [Delayed examples](https://examples.dask.org/delayed.html)\n",
    "* [Delayed best practices](https://docs.dask.org/en/latest/delayed-best-practices.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1709cfd7-2e92-450e-a6c4-05c289f3dbed",
   "metadata": {},
   "source": [
    "As we'll see in the [distributed scheduler notebook](05_distributed.ipynb), Dask has several ways of executing code in parallel. We'll use the distributed scheduler by creating a `dask.distributed.Client`. For now, this will provide us with some nice diagnostics. We'll talk about schedulers in depth later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa63b73-40b7-4875-8fd6-733afafd021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse client\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfd29c4-384d-4977-a05e-a1d7d6db4864",
   "metadata": {},
   "source": [
    "## A Typical Workflow\n",
    "\n",
    "Typically if a workflow contains a for-loop it can benefit from delayed. The following example outlines a read-transform-write:\n",
    "\n",
    "```python\n",
    "import dask\n",
    "    \n",
    "@dask.delayed\n",
    "def process_file(filename):\n",
    "    data = read_a_file(filename)\n",
    "    data = do_a_transformation(data)\n",
    "    destination = f\"results/{filename}\"\n",
    "    write_out_data(data, destination)\n",
    "    return destination\n",
    "\n",
    "results = []\n",
    "for filename in filenames:\n",
    "    results.append(process_file(filename))\n",
    "    \n",
    "dask.compute(results)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8113af4d-ae4b-4347-98a1-d1c3a91f7365",
   "metadata": {},
   "source": [
    "## Basics\n",
    "\n",
    "First let's make some toy functions, `inc` and `add`, that sleep for a while to simulate work. We'll then time running these functions normally.\n",
    "\n",
    "In the next section we'll parallelize this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f70b6d4-9fb3-40e4-a97a-86ac5fc74cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "\n",
    "def inc(x):\n",
    "    sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "def add(x, y):\n",
    "    sleep(1)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bae723-13cb-4c5f-b03d-f1a49ab29802",
   "metadata": {},
   "source": [
    "We time the execution of this normal code using the `%%time` magic, which is a special function of the Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331fb6d2-1858-4642-9035-f0b659cc04b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This takes three seconds to run because we call each\n",
    "# function sequentially, one after the other\n",
    "\n",
    "x = inc(1)\n",
    "y = inc(2)\n",
    "z = add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05bc686-be14-4fee-9935-64d317a46e54",
   "metadata": {},
   "source": [
    "### Parallelize with the `dask.delayed` decorator\n",
    "\n",
    "Those two increment calls *could* be called in parallel, because they are totally independent of one-another.\n",
    "\n",
    "We'll make the `inc` and `add` functions lazy using the `dask.delayed` decorator. When we call the delayed version by passing the arguments, exactly as before, the original function isn't actually called yet - which is why the cell execution finishes very quickly.\n",
    "Instead, a *delayed object* is made, which keeps track of the function to call and the arguments to pass to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dac6f2b-0af1-4c81-8c70-7de370a39db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def inc(x):\n",
    "    sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def add(x, y):\n",
    "    sleep(1)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7930129d-eeab-4110-b82f-9945cedff950",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This runs immediately, all it does is build a graph\n",
    "\n",
    "x = inc(1)\n",
    "y = inc(2)\n",
    "z = add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b782da-eab9-4d0a-b9bf-1ac6f71b6419",
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9c327b-360e-4cba-80af-9b6494cc7b5c",
   "metadata": {},
   "source": [
    "This ran immediately, since nothing has really happened yet.\n",
    "\n",
    "To get the result, call `compute`. Notice that this runs faster than the original code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b816c53-5c43-47d2-a191-3667f5b74097",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This actually runs our computation using a local thread pool\n",
    "\n",
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7828fd60-6327-4997-a623-97e65621da3d",
   "metadata": {},
   "source": [
    "## What just happened?\n",
    "\n",
    "The `z` object is a lazy `Delayed` object.  This object holds everything we need to compute the final result, including references to all of the functions that are required and their inputs and relationship to one-another.  We can evaluate the result with `.compute()` as above or we can visualize the task graph for this value with `.visualize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c590a4d9-47a1-48fc-9eaf-e375b9c77cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb4d013-d17d-4cd0-a1a1-7dacfc88388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the task graph for `z`\n",
    "z.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad12e6d-b2b0-4412-af3c-cc6fd1a6dbcf",
   "metadata": {},
   "source": [
    "Notice that this includes the names of the functions from before, and the logical flow of the outputs of the `inc` functions to the inputs of `add`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275367a5-c778-4392-9d3c-8991d38604e3",
   "metadata": {},
   "source": [
    "### Some questions to consider:\n",
    "\n",
    "-  Why did we go from 3s to 2s?  Why weren't we able to parallelize down to 1s?\n",
    "-  What would have happened if the inc and add functions didn't include the `sleep(1)`?  Would Dask still be able to speed up this code?\n",
    "-  What if we have multiple outputs or also want to get access to x or y?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13936566-6eea-4dcd-b327-6bfb46f3e119",
   "metadata": {},
   "source": [
    "## Exercise: Parallelize a for loop\n",
    "\n",
    "`for` loops are one of the most common things that we want to parallelize.  Use `dask.delayed` on `inc` and `sum` to parallelize the computation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d32682-896d-4e4d-bfe5-65c84cd86704",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5, 6, 7, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304cb046-0139-48c1-bf5e-faa78126d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Sequential code\n",
    "\n",
    "\n",
    "def inc(x):\n",
    "    sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "results = []\n",
    "for x in data:\n",
    "    y = inc(x)\n",
    "    results.append(y)\n",
    "\n",
    "total = sum(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68340cfa-86d4-4c45-be63-2d48c4bffcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05022296-c011-4926-81af-99277b8685c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Your parallel code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99f8b40-25f6-481f-82da-65f3c816284d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "@dask.delayed\n",
    "def inc(x):\n",
    "    sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "@dask.delayed\n",
    "def delay_sum(x):\n",
    "    return sum(x)\n",
    "\n",
    "results = []\n",
    "for x in data:\n",
    "    y = inc(x)\n",
    "    results.append(y)\n",
    "\n",
    "total = delay_sum(results)\n",
    "print(\"Before computing:\", total)  # Let's see what type of thing total is\n",
    "result = total.compute()\n",
    "print(\"After computing :\", result)  # After it's computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46195ca3-af84-4668-8e07-42d05329c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def inc(x):\n",
    "    sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "results = []\n",
    "for x in data:\n",
    "    y = inc(x)\n",
    "    results.append(y)\n",
    "\n",
    "total = sum(results)\n",
    "print(\"Before computing:\", total)  # Let's see what type of thing total is\n",
    "result = total.compute()\n",
    "print(\"After computing :\", result)  # After it's computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96637ba1-baa5-49b5-864f-1a5c4c4ab90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795244dd-320f-441e-9eb4-c098334b0e51",
   "metadata": {},
   "source": [
    "How does the above graph visualization compare with the given solution? That is, with the `sum` function used directly rather than wrapped with `delayed`? Can you explain the why they are different? You might find the graph visualizations of the following expressions illuminating\n",
    "\n",
    "```python\n",
    "inc(1) + inc(2)\n",
    "# and\n",
    "sum([inc(1), inc(2)])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2105dbdb-363c-4eec-8530-8e70113b6e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(inc(1) + inc(2)).visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f502bc-b332-451e-9e0e-d2ac5da97ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(sum([inc(1), inc(2)])).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcf29bf-466a-4929-9817-e9810989e19e",
   "metadata": {},
   "source": [
    "## Exercise: Parallelize a for-loop code with control flow\n",
    "\n",
    "Often we want to delay only *some* functions, running a few of them immediately.  This is especially helpful when those functions are fast and help us to determine what other slower functions we should call.  This decision, to delay or not to delay, is usually where we need to be thoughtful when using `dask.delayed`.\n",
    "\n",
    "In the example below we iterate through a list of inputs.  If that input is even then we want to call `inc`.  If the input is odd then we want to call `double`.  This `is_even` decision to call `inc` or `double` has to be made immediately (not lazily) in order for our graph-building Python code to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b650e793-760a-4403-9fbb-889b85a20968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double(x):\n",
    "    sleep(1)\n",
    "    return 2 * x\n",
    "\n",
    "\n",
    "def is_even(x):\n",
    "    return not x % 2\n",
    "\n",
    "\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5075253f-d905-4f6a-af23-65d85dd54924",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Sequential code\n",
    "\n",
    "results = []\n",
    "for x in data:\n",
    "    if is_even(x):\n",
    "        y = double(x)\n",
    "    else:\n",
    "        y = inc(x)\n",
    "    results.append(y)\n",
    "\n",
    "total = sum(results)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d67ede8-7baf-483c-ac91-0b60c22f8b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Your parallel code here...\n",
    "# TODO: parallelize the sequential code above using dask.delayed\n",
    "# You will need to delay some functions, but not all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69e9190-c9ee-45ce-a80a-f459e95d7e2a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "@dask.delayed\n",
    "def double(x):\n",
    "    sleep(1)\n",
    "    return 2 * x\n",
    "\n",
    "\n",
    "results = []\n",
    "for x in data:\n",
    "    if is_even(x):  # even\n",
    "        y = double(x)\n",
    "    else:  # odd\n",
    "        y = inc(x)\n",
    "    results.append(y)\n",
    "\n",
    "total = dask.delayed(sum)(results)\n",
    "total.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a42e750-3690-48b2-88da-5344d60aa39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839e1726-8779-42b8-9a68-be133f48cee5",
   "metadata": {},
   "source": [
    "### Some questions to consider:\n",
    "\n",
    "-  What are other examples of control flow where we can't use delayed?\n",
    "-  What would have happened if we had delayed the evaluation of `is_even(x)` in the example above?\n",
    "-  What are your thoughts on delaying `sum`?  This function is both computational but also fast to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097c1a6a-734e-4b29-bbaa-2d4e4db0f2d6",
   "metadata": {},
   "source": [
    "## Exercise: Parallelize a Pandas Groupby Reduction\n",
    "\n",
    "In this exercise we read several CSV files and perform a groupby operation in parallel.  We are given sequential code to do this and parallelize it with `dask.delayed`.\n",
    "\n",
    "The computation we will parallelize is to compute the mean departure delay per airport from some historical flight data.  We will do this by using `dask.delayed` together with `pandas`.  In a future section we will do this same exercise with `dask.dataframe`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a9311-1831-4d58-8e56-bf2481dd27a4",
   "metadata": {},
   "source": [
    "### Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c3bbb-b323-406d-b5b6-c6b01c2349ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "sorted(os.listdir(os.path.join(\"data\", \"nycflights\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eba07be-4406-499d-a216-89c562bf5844",
   "metadata": {},
   "source": [
    "### Read one file with `pandas.read_csv` and compute mean departure delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525f5fb1-6e9f-44b7-8c06-8d5881bf097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(os.path.join(\"data\", \"nycflights\", \"1990.csv\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43547154-752a-434b-8fe5-07cd9348470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the schema?\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151f9d5a-aa3b-4272-87be-41d84a6a700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What originating airports are in the data?\n",
    "df.Origin.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbea82b-92f9-4b1c-a1ee-4ed2d1f85912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean departure delay per-airport for one year\n",
    "df.groupby(\"Origin\").DepDelay.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f772954b-fbd7-4332-9a66-3de13d8c0c62",
   "metadata": {},
   "source": [
    "### Sequential code: Mean Departure Delay Per Airport\n",
    "\n",
    "The above cell computes the mean departure delay per-airport for one year. Here we expand that to all years using a sequential for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea45bdb-8138-424a-9615-078567417696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "filenames = sorted(glob(os.path.join(\"data\", \"nycflights\", \"*.csv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e35814c-0ce7-4347-9b57-272f5096d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sums = []\n",
    "counts = []\n",
    "for fn in filenames:\n",
    "    # Read in file\n",
    "    df = pd.read_csv(fn)\n",
    "\n",
    "    # Groupby origin airport\n",
    "    by_origin = df.groupby(\"Origin\")\n",
    "\n",
    "    # Sum of all departure delays by origin\n",
    "    total = by_origin.DepDelay.sum()\n",
    "\n",
    "    # Number of flights by origin\n",
    "    count = by_origin.DepDelay.count()\n",
    "\n",
    "    # Save the intermediates\n",
    "    sums.append(total)\n",
    "    counts.append(count)\n",
    "\n",
    "# Combine intermediates to get total mean-delay-per-origin\n",
    "total_delays = sum(sums)\n",
    "n_flights = sum(counts)\n",
    "mean = total_delays / n_flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec040042-13b3-49a0-92d1-8cbacde8e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5216045-c516-486e-a53c-a30a85a3f992",
   "metadata": {},
   "source": [
    "### Parallelize the code above\n",
    "\n",
    "Use `dask.delayed` to parallelize the code above.  Some extra things you will need to know.\n",
    "\n",
    "1.  Methods and attribute access on delayed objects work automatically, so if you have a delayed object you can perform normal arithmetic, slicing, and method calls on it and it will produce the correct delayed calls.\n",
    "    \n",
    "2.  Calling the `.compute()` method works well when you have a single output.  When you have multiple outputs you might want to use the `dask.compute` function. This way Dask can share the intermediate values.\n",
    "    \n",
    "So your goal is to parallelize the code above (which has been copied below) using `dask.delayed`.  You may also want to visualize a bit of the computation to see if you're doing it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4d857b-32b4-449f-841c-4fe509478cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00f5ae0-a7ef-479c-b025-1d0f25816f43",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "# This is just one possible solution, there are\n",
    "# several ways to do this using `dask.delayed`\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def read_file(filename):\n",
    "    # Read in file\n",
    "    return pd.read_csv(filename)\n",
    "\n",
    "\n",
    "sums = []\n",
    "counts = []\n",
    "for fn in filenames:\n",
    "    # Delayed read in file\n",
    "    df = read_file(fn)\n",
    "\n",
    "    # Groupby origin airport\n",
    "    by_origin = df.groupby(\"Origin\")\n",
    "\n",
    "    # Sum of all departure delays by origin\n",
    "    total = by_origin.DepDelay.sum()\n",
    "\n",
    "    # Number of flights by origin\n",
    "    count = by_origin.DepDelay.count()\n",
    "\n",
    "    # Save the intermediates\n",
    "    sums.append(total)\n",
    "    counts.append(count)\n",
    "\n",
    "# Combine intermediates to get total mean-delay-per-origin\n",
    "total_delays = sum(sums)\n",
    "n_flights = sum(counts)\n",
    "mean, *_ = dask.compute(total_delays / n_flights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702da65a-7208-434f-b83d-e4ce903c7c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(total_delays / n_flights).visualize(engine=\"cytoscape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e63df2a-df45-42ad-b205-812dcfb22696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the results still match\n",
    "mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a129e01c-6b28-454c-b78b-e3b8ba897b3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Some questions to consider:\n",
    "\n",
    "- How much speedup did you get? Is this how much speedup you'd expect?\n",
    "- Experiment with where to call `compute`. What happens when you call it on `sums` and `counts`? What happens if you wait and call it on `mean`?\n",
    "- Experiment with delaying the call to `sum`. What does the graph look like if `sum` is delayed? What does the graph look like if it isn't?\n",
    "- Can you think of any reason why you'd want to do the reduction one way over the other?\n",
    "\n",
    "### Learn More\n",
    "\n",
    "Visit the [Delayed documentation](https://docs.dask.org/en/latest/delayed.html). In particular, this [delayed screencast](https://www.youtube.com/watch?v=SHqFmynRxVU) will reinforce the concepts you learned here and the [delayed best practices](https://docs.dask.org/en/latest/delayed-best-practices.html) document collects advice on using `dask.delayed` well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43292438-7c3a-42c9-8b61-106889199e45",
   "metadata": {},
   "source": [
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\">\n",
    "\n",
    "# Distributed - spread your data and computation across a cluster\n",
    "\n",
    "As we covered at the beginning Dask has the ability to run work on multiple machines using the distributed scheduler.\n",
    "\n",
    "Until now we have actually been using the distributed scheduler for our work, but just on a single machine.\n",
    "\n",
    "When we instantiate a `Client()` object with no arguments it will attempt to locate a Dask cluster. It will check your local Dask config and environment variables to see if connection information has been specified. If not it will create an instance of `LocalCluster` and use that.\n",
    "\n",
    "*Specifying connection information in config is useful for system administrators to provide access to their users. We do this in the [Dask Helm Chart for Kubernetes](https://github.com/dask/helm-chart/blob/master/dask/templates/dask-jupyter-deployment.yaml#L46-L48), the chart installs a multi-node Dask cluster and a Jupyter server on a Kubernetes cluster and Jupyter is preconfigured to discover the distributed cluster.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a08054-44f0-4189-a4d7-2fc96d9e0e3d",
   "metadata": {},
   "source": [
    "## Local Cluster\n",
    "\n",
    "Let's explore the `LocalCluster` object ourselves and see what it is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66b5d63-bd18-4fde-8156-8fbda65dacd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster, Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d6ef1-bb4a-45e3-95c9-c7eca84e709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove our original client + cluster\n",
    "client.shutdown()\n",
    "del(client)\n",
    "\n",
    "cluster = LocalCluster()\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9490396-35be-4752-a57c-a876dd7e6dc0",
   "metadata": {},
   "source": [
    "Creating a cluster object will create a Dask scheduler and a number of Dask workers. If no arguments are specified then it will autodetect the number of CPU cores your system has and the amount of memory and create workers to appropriately fill that.\n",
    "\n",
    "You can also specify these arguments yourself. Let's have a look at the docstring to see the options we have available.\n",
    "\n",
    "*These arguments can also be passed to `Client` and in the case where it creates a `LocalCluster` they will just be passed on down the line.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922881f4-0163-4c19-9719-40df57f2abeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "?LocalCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e934b487-e626-4537-a219-b333c1fde83f",
   "metadata": {},
   "source": [
    "Our cluster object has attributes and methods which we can use to access information about our cluster. For instance we can get the log output from the scheduler and all the workers with the `get_logs()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a4cd04-02e6-4d06-8d46-4147328c454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970efff4-404e-4472-8eab-b8adcbce9ebe",
   "metadata": {},
   "source": [
    "We can access the url that the Dask dashboard is being hosted at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a5840d-82c6-4e96-9a72-7d3ee4c58922",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.dashboard_link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6801a67-ef9e-4dbd-bb6c-43c5030e3a8a",
   "metadata": {},
   "source": [
    "In order for Dask to use our cluster we still need to create a `Client` object, but as we have already created a cluster we can pass that directly to our client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca9c6fd-299e-4002-98ad-f3cf96ccb2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77448299-5c34-4218-a083-db0c3cdd11b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown for next example\n",
    "client.shutdown()\n",
    "del client, cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4e535e-a00c-4c24-8f1d-9dca965bf73a",
   "metadata": {},
   "source": [
    "## Remote clusters via SSH\n",
    "\n",
    "A common way to distribute your work onto multiple machines is via SSH. Dask has a cluster manager which will handle creating SSH connections for you called `SSHCluster`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f327d5c-d1ef-4622-90f1-94c01ea31c57",
   "metadata": {},
   "source": [
    "```python\n",
    "from dask.distributed import SSHCluster\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e8868d-5a61-4664-b3bd-5228a86473be",
   "metadata": {},
   "source": [
    "When constructing this cluster manager we need to pass a list of addresses, either hostnames or IP addresses, which we will SSH into and attempt to start a Dask scheduler or worker on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f3a025-2ec7-4e42-b5e4-763d0d3de90a",
   "metadata": {},
   "source": [
    "```python\n",
    "cluster = SSHCluster([\"localhost\", \"hostA\", \"hostB\"])\n",
    "cluster\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cdbbee-4055-43d6-94a2-e510f85ed991",
   "metadata": {},
   "source": [
    "When we create our `SSHCluster` object we have given a list of three hostnames.\n",
    "\n",
    "The first host in the list will be used as the scheduler, all other hosts will be used as workers. If you're on the same network it wouldn't be unreasonable to set your local machine as the scheduler and then use other machines as workers.\n",
    "\n",
    "If your servers are remote to you, in the cloud for instance, you may want the scheduler to be a remote machine too to avoid network bottlenecks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e436ea1f-b007-47e0-b7d5-96d912e849d0",
   "metadata": {},
   "source": [
    "## Scalable clusters\n",
    "\n",
    "Both of the clusters we have seen so far are fixed size clusters. We are either running locally and using all the resources in our machine, or we are using an explicit number of other machines via SSH.\n",
    "\n",
    "With some cluster managers it is possible to increase and descrease the number of workers either by calling `cluster.scale(n)` in your code where `n` is the desired number of workers. Or you can let Dask do this dynamically by calling `cluster.adapt(minimum=1, maximum=100)` where minimum and maximum are your preferred limits for Dask to abide to.\n",
    "\n",
    "It is always good to keep your minimum to at least 1 as Dask will start running work on a single worker in order to profile how long things take and extrapolate how many additional workers it thinks it needs. Getting new workers may take time depending on your setup so keeping this at 1 or above means this profilling will start immediately.\n",
    "\n",
    "We currently have cluster managers for [Kubernetes](https://kubernetes.dask.org/en/latest/), [Hadoop/Yarn](https://yarn.dask.org/en/latest/), [cloud platforms](https://cloudprovider.dask.org/en/latest/) and [batch systems including PBS, SLURM and SGE](http://jobqueue.dask.org/en/latest/).\n",
    "\n",
    "These cluster managers allow users who have access to resources such as these to bootstrap Dask clusters on to them. If an institution wishes to provide a central service that users can request Dask clusters from there is also [Dask Gateway](https://gateway.dask.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e629e1-1e43-4f21-8dad-ccde9a466ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "cluster = SLURMCluster(cores=8,\n",
    "                       memory=\"16GB\",\n",
    "                       walltime=\"01:00:00\",\n",
    "                       queue=\"normal\",\n",
    "                       interface=\"ib0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3057d726-d709-4525-8d31-742a021e8194",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb3a078-59d9-4eee-94f2-02a7fcd8e18a",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "You may have noticed that we have zero workers. Let's ask for one worker. This will give us the resources we requested in the `SLURMCluster` call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739092c7-9b3e-4a22-b9f8-f77f3ff1fa6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster.scale(1)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b2f0a1-1e43-4e86-9922-4595b9cf0a41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We could also do this adaptively with `cluster.adapt`\n",
    "cluster.adapt?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f5050b-6045-4b14-8b81-e2e92d6455e9",
   "metadata": {},
   "source": [
    "Dask will automatically write and submit a SLURM jobscript for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea5bfa5-f503-43c5-ae56-c60e9fc5fef9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32a92ff-bc30-4bb5-84d6-5007b4385944",
   "metadata": {},
   "source": [
    "As an example, let's compute the same mean of the large array from earlier using our SLURM cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeec978",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "xd = da.random.normal(10, 0.1, size=(30_000, 30_000), chunks=(3000, 3000))\n",
    "yd = xd.mean(axis=0)\n",
    "yd.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17254476-6968-48b7-a4b9-62713aaeaeac",
   "metadata": {},
   "source": [
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\">\n",
    "\n",
    "# Futures - non-blocking distributed calculations\n",
    "\n",
    "Submit arbitrary functions for computation in a parallelized, eager, and non-blocking way. \n",
    "\n",
    "The `futures` interface (derived from the built-in `concurrent.futures`) provide fine-grained real-time execution for custom situations. We can submit individual functions for evaluation with one set of inputs, or evaluated over a sequence of inputs with `submit()` and `map()`. The call returns immediately, giving one or more *futures*, whose status begins as \"pending\" and later becomes \"finished\". There is no blocking of the local Python session.\n",
    "\n",
    "This is the important difference between futures and delayed. Both can be used to support arbitrary task scheduling, but delayed is lazy (it just constructs a graph) whereas futures are eager. With futures, as soon as the inputs are available and there is compute available, the computation starts. \n",
    "\n",
    "**Related Documentation**\n",
    "\n",
    "* [Futures documentation](https://docs.dask.org/en/latest/futures.html)\n",
    "* [Futures screencast](https://www.youtube.com/watch?v=07EiCpdhtDE)\n",
    "* [Futures examples](https://examples.dask.org/futures.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4b3429-3bc7-4e0d-a6ac-3b80c01c477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse client\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f644e393-ef92-4bb5-b328-776409ee59fb",
   "metadata": {},
   "source": [
    "## A Typical Workflow\n",
    "\n",
    "This is the same workflow that we saw in the delayed notebook. It is for-loopy and the data is not necessarily an array or a dataframe. The following example outlines a read-transform-write:\n",
    "\n",
    "```python\n",
    "def process_file(filename):\n",
    "    data = read_a_file(filename)\n",
    "    data = do_a_transformation(data)\n",
    "    destination = f\"results/{filename}\"\n",
    "    write_out_data(data, destination)\n",
    "    return destination\n",
    "\n",
    "futures = []\n",
    "for filename in filenames:\n",
    "    future = client.submit(process_file, filename)\n",
    "    futures.append(future)\n",
    "    \n",
    "futures\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eaff17-2d74-4da3-a07a-b15d3fab35e1",
   "metadata": {},
   "source": [
    "## Basics\n",
    "\n",
    "Just like we did in the delayed notebook, let's make some toy functions, `inc` and `add`, that sleep for a while to simulate work. We'll then time running these functions normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefac0ea-4256-41a4-a60d-b838d374f3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "\n",
    "def inc(x):\n",
    "    sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "def double(x):\n",
    "    sleep(2)\n",
    "    return 2 * x\n",
    "\n",
    "\n",
    "def add(x, y):\n",
    "    sleep(1)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a174e439-0835-446a-bad7-3ee3d770644f",
   "metadata": {},
   "source": [
    "We can run these locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf42befc-f32a-4685-a36f-fdb1c4fc4dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "rng = da.random.default_rng()\n",
    "vals = rng.standard_normal(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2115f9-1d79-4b79-805a-13f487375dcb",
   "metadata": {},
   "source": [
    "Or we can submit them to run remotely with Dask. This immediately returns a future that points to the ongoing computation, and eventually to the stored result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf21389-1dfd-4816-b25c-d883bcd18082",
   "metadata": {},
   "outputs": [],
   "source": [
    "future = client.submit(inc, 1)  # returns immediately with pending future\n",
    "future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d43fde4-6af4-4d82-b1ff-b41224edf598",
   "metadata": {},
   "source": [
    "If you wait a second, and then check on the future again, you’ll see that it has finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c29be3-7020-447b-bdc5-d645612bb3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbbaa41-e7d9-4f92-8a3a-551a6a47ff73",
   "metadata": {},
   "source": [
    "You can block on the computation and gather the result with the `.result()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef31fa4b-e970-4208-91fb-76562e7611ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8257579-737b-4bbf-bf67-86d6e93b92db",
   "metadata": {},
   "source": [
    "#### Other ways to wait for a future\n",
    "```python\n",
    "from dask.distributed import wait, progress\n",
    "progress(future)\n",
    "```\n",
    "\n",
    "shows a progress bar in *this* notebook, rather than having to go to the dashboard. This progress bar is also asynchronous, and doesn't block the execution of other code in the meanwhile.\n",
    "\n",
    "```python\n",
    "wait(future)\n",
    "```\n",
    "blocks and forces the notebook to wait until the computation pointed to by `future` is done. However, note that if the result of `inc()` is sitting in the cluster, it would take **no time** to execute the computation now, because Dask notices that we are asking for the result of a computation it already knows about. More on this later.\n",
    "\n",
    "#### Other ways to gather results\n",
    "```python\n",
    "client.gather(futures)\n",
    "```\n",
    "\n",
    "gathers results from more than one future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95ec50f-f4a3-4cfc-a9c2-2ed153938616",
   "metadata": {},
   "source": [
    "## `client.compute`\n",
    "\n",
    "Generally, any Dask operation that is executed using `.compute()` or `dask.compute()` can be submitted for asynchronous execution using `client.compute()` instead.\n",
    "\n",
    "Here is an example from the delayed notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3964c-35fb-4b48-bc2e-716f422092e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "rng = da.random.default_rng()\n",
    "vals = rng.standard_normal(10)\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def inc(x):\n",
    "    sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def add(x, y):\n",
    "    sleep(1)\n",
    "    return x + y\n",
    "\n",
    "\n",
    "x = inc(1)\n",
    "y = inc(2)\n",
    "z = add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543b0cac-d811-419c-b339-1d08faf345d0",
   "metadata": {},
   "source": [
    "So far we have a regular `dask.delayed` output. When we pass `z` to `client.compute` we get a future back and Dask starts evaluating the task graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be9e068-2268-43cd-a5ed-b8a8e993975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the difference from z.compute()\n",
    "# notice that this cell completes immediately\n",
    "future = client.compute(z)\n",
    "future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afa795a-3747-47f2-b52d-025f1566acf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "future.result()  # waits until result is ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3388e35b-37dd-427c-8b48-29a94bc6c1ed",
   "metadata": {},
   "source": [
    "When using futures, the *computation moves to the data* rather than the other way around, and the client, in the local Python session, need never see the intermediate values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a29d581-3204-421c-9f50-4ea7d49ced3e",
   "metadata": {},
   "source": [
    "## `client.submit`\n",
    "\n",
    "`client.submit` takes a function and arguments, pushes these to the cluster, returning a `Future` representing the result to be computed. The function is passed to a worker process for evaluation. This looks a lot like doing `client.compute()`, above, except now we are passing the function and arguments directly to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3abf38-d760-42f9-be27-caf0613bd89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc(x):\n",
    "    sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "future_x = client.submit(inc, 1)\n",
    "future_y = client.submit(inc, 2)\n",
    "future_z = client.submit(sum, [future_x, future_y])\n",
    "future_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5496c93c-7e3f-4f3c-982f-e28b35d46328",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_z.result()  # waits until result is ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24355057-7c69-4452-a509-88ca6c3bb72f",
   "metadata": {},
   "source": [
    "The arguments to`client.submit` can be regular Python functions and objects, futures from other submit operations or `dask.delayed` objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2874d138-25ba-43a4-94ba-ed62eed7d5ba",
   "metadata": {},
   "source": [
    "### How does it work?\n",
    "\n",
    "Each future represents a result held, or being evaluated by the cluster. Thus we can control caching of intermediate values - when a future is no longer referenced, its value is forgotten. In the solution, above, futures are held for each of the function calls. These results would not need to be re-evaluated if we chose to submit more work that needed them.\n",
    "\n",
    "We can explicitly pass data from our local session into the cluster using `client.scatter()`, but usually it is better to construct functions that do the loading of data within the workers themselves, so that there is no need to serialize and communicate the data. Most of the loading functions within Dask, such as `dd.read_csv`, work this way. Similarly, we normally don't want to `gather()` results that are too big in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50da7ff0-69be-4805-a021-86f6a4023167",
   "metadata": {},
   "source": [
    "## Example: Sporadically failing task\n",
    "\n",
    "Let's imagine a task that sometimes fails. You might encounter this when dealing with input data where sometimes a file is malformed, or maybe a request times out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a08da-ece9-4503-be7e-50793aec370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flaky_inc(i):\n",
    "    val = np.random.random(1)\n",
    "    print(val)\n",
    "    if val < 0.5:\n",
    "        raise ValueError(f\"You hit the error {val=}!\")\n",
    "    return i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe23f4f4-eb3b-45b9-a73c-2600db5215ee",
   "metadata": {},
   "source": [
    "If you run this function over and over again, it will sometimes fail. \n",
    "\n",
    "```python\n",
    ">>> flaky_inc(2)\n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "Input In [65], in <cell line: 1>()\n",
    "----> 1 flaky_inc(2)\n",
    "\n",
    "Input In [61], in flaky_inc(i)\n",
    "      3 def flaky_inc(i):\n",
    "      4     if random() < 0.5:\n",
    "----> 5         raise ValueError(\"You hit the error!\")\n",
    "      6     return i + 1\n",
    "\n",
    "ValueError: You hit the error!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246d164b-9fe0-43a4-8353-2d122e1fab33",
   "metadata": {},
   "source": [
    "We can run this function on a range of inputs using `client.map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c90250b-efab-44b8-8920-726823056f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = client.map(flaky_inc, range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6903c7f-296f-49c5-a77f-f201bc0aca0a",
   "metadata": {},
   "source": [
    "Notice how the cell returned even though some of the computations failed. We can inspect these futures one by one and find the ones that failed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b02613-21f4-418b-ad59-55f0a68755f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, future in enumerate(futures):\n",
    "    print(i, future.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb2edd5-3b0e-4ebc-b7cd-20259bda3b80",
   "metadata": {},
   "source": [
    "You can rerun those specific futures to try to get the task to successfully complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba3ed52-d871-4d17-a62b-814772d9eae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "futures[3].retry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5958434-cd2d-40cc-8dab-3e21a5f9759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, future in enumerate(futures):\n",
    "    print(i, future.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cbbca9-ba6c-4cef-99a6-461d2ba437f5",
   "metadata": {},
   "source": [
    "A more concise way of retrying in the case of sporadic failures is by setting the number of retries in the `client.compute`, `client.submit` or `client.map` method.\n",
    "\n",
    "**Note**: In this example we also need to set `pure=False` to let Dask know that the arguments to the function do not totally determine the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a09954c-3c43-4ac1-9dbb-c372cd458a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = client.map(flaky_inc, range(10), retries=5, pure=False)\n",
    "future_z = client.submit(sum, futures)\n",
    "future_z.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f2003-2d7a-49d1-941a-86d17d295e7c",
   "metadata": {},
   "source": [
    "You will see a lot of warnings, but the computation should eventually succeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697587ce-8cb7-4e39-a5d9-23353deca57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, future in enumerate(futures):\n",
    "    print(i, future.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3cc144-c77c-4286-803a-d6088d3842cb",
   "metadata": {},
   "source": [
    "## Why use Futures?\n",
    "\n",
    "The futures API offers a work submission style that can easily emulate the map/reduce paradigm. If that is familiar to you then futures might be the simplest entrypoint into Dask. \n",
    "\n",
    "The other big benefit of futures is that the intermediate results, represented by futures, can be passed to new tasks without having to pull data locally from the cluster. New operations can be setup to work on the output of previous jobs that haven't even begun yet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
